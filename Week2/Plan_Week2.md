# Week 2: Advanced Multi-Armed Bandit Techniques and Markov Decision Processes (Dec 17th - Dec 23rd)

Welcome to Week 2 of the Reinforcement Learning program! This week, we’ll delve deeper into the **Multi-Armed Bandit (MAB)** problem, exploring advanced techniques, and start learning about **Markov Decision Processes (MDPs)**, a foundational concept in reinforcement learning.

## Learning Plan

### 1. Multi-Armed Bandit (MAB) Advanced Techniques
- **Objective**: Build on last week's understanding of MAB and explore advanced strategies for solving the problem.
- **Techniques Covered**:
  - **Epsilon-Greedy**
  - **Upper Confidence Bound (UCB)**
  - **Thompson Sampling**
- **Resources** (same as Week 1):
  - **[Lecture 9](../Resources/David%20Silver%20Slides/lec9.pdf)** from David Silver’s RL Slides (`lec9.pdf`)
  - **[Chapter 2](../Resources/SuttonBartoIPRLBook2ndEd.pdf)** of *Reinforcement Learning: An Introduction* by Sutton & Barto (`SuttonBartoIPRLBook2ndEd.pdf`)
  - **Additional Reading**: External [article](https://lilianweng.github.io/posts/2018-01-23-multi-armed-bandit/)
 
**Note:** Once you are done with the Reading of Multi-Armed Bandits, you can proceed to solve this week's assignment.

### 2. Markov Decision Processes (MDPs)
- **Objective**: Understand the basic structure of MDPs and their importance in RL.
- **Key Topics**:
  - States, actions, and rewards
  - Transition probabilities
  - Value functions
- **Resources**:
  - **[Lecture 2](../Resources/David%20Silver%20Slides/lec2.pdf)** from David Silver’s RL Slides (`lec2.pdf`)
  - **[Chapter 3](../Resources/SuttonBartoIPRLBook2ndEd.pdf)** of *Reinforcement Learning: An Introduction* by Sutton & Barto (`SuttonBartoIPRLBook2ndEd.pdf`)
  - **[Section 1 & 2](../Resources/RLAlgsInMDPs.pdf)** from `RLAlgsInMDPs.pdf`

### 3. Assignment
- **Task**: Implement and analyze different MAB techniques (Epsilon-Greedy, UCB, and Thompson Sampling) in the provided notebook:
  - Compare the performance of these techniques on various bandit scenarios.
  - Analyze how parameters like epsilon (for Epsilon-Greedy) and confidence bounds (for UCB) affect the results.
