# Week 3: Dynamic Programming - Concepts and Implementation (Dec 24th - Dec 30th)

Welcome to Week 3 of the Reinforcement Learning program! This week, we venture into **Dynamic Programming (DP)**, a powerful approach for solving Markov Decision Processes (MDPs). The focus will be on learning core DP techniques and implementing two classic problems.

## Learning Plan

### 1. Introduction to Dynamic Programming
- **Objective**: Understand the foundational concepts of dynamic programming for solving MDPs.
- **Key Topics**:
  - Policy Evaluation
  - Policy Improvement
  - Policy Iteration
  - Value Iteration
- **Resources**:
  - **[Chapter 4](../Resources/SuttonBartoIPRLBook2ndEd.pdf)** of *Reinforcement Learning: An Introduction* by Sutton & Barto (`SuttonBartoIPRLBook2ndEd.pdf`)
  - **[Lecture 3](../Resources/David%20Silver%20Slides/lec3.pdf)** from David Silver’s RL Slides (`lec3.pdf`)

### 2. Assignment: Solving Classic Problems with DP
- **Task**: Implement and analyze solutions for:
  1. **Gambler’s Problem**:
     - Problem description: A gambler has the chance to double their stake with each bet, subject to a success probability, and aims to reach a goal amount.
     - Focus: Policy iteration and value iteration methods.
  2. **Jack’s Car Rental Problem**:
     - Problem description: Jack manages two rental locations, balancing the transfer of cars between them to maximize profit.
     - Focus: Policy iteration and exploring computational trade-offs in DP solutions.
- **Assignment Files**:
  - `GamblersProblem.ipynb`
  - `JacksCarRental.ipynb`
